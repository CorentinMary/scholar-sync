# evaluation pipeline
import datetime as dt
import json
import logging
import tempfile
import time
from argparse import ArgumentParser
from typing import List

import mlflow
import pandas as pd
import yaml
from dotenv import load_dotenv
from tqdm import tqdm

from src.data_preprocessing import download_documents
from src.evaluation import evaluate, generate_dataset
from src.inference import InferencePipeline

# The pipeline can be run in two modes:
# - self-supervised, where the ground truth to evaluate the prediction against will be randomly generated by sampling
# the documents database,
# - supervised, where a ground truth dataset should be provided as an argument.
AVAILABLE_MODES = ["self-supervised", "supervised"]

# Logging configuration
datetime_str = dt.datetime.now().strftime(format="%Y-%m-%d_%H%M")
log_file_name = f"./logs/eval_{datetime_str}.log"
logging.basicConfig(
    filename=log_file_name,
    level=logging.INFO,
)

# Loading environment variables and configuration
load_dotenv()
config = yaml.safe_load(open("./config.yaml", "r"))


def evaluation_pipeline(
    retriever_name: str,
    summarizer_name: str,
    n_sim_docs: int,
    max_summary_tokens: int,
    metrics: List[str],
    mode: str,
    size: int = 10,
    n_sample_sentences: int = 2,
    dataset_path: str = None,
    experiment_id: str = "0",
) -> None:
    """Pipeline that:
     - downloads the documents database,
     - creates (or loads) the evalaution dataset,
     - initializes the inference pipeline,
     - runs the inference and evaluates it on all items of the dataset,
     - stores the evaluation results and artifacts with mlflow.

    :param retriever_name: str
        name of the retriever model to evaluate. Should be one of src.inference.RETRIEVAL_DICT's keys.
    :param summarizer_name: str
        name the summarizer model to evaluate. Should be one of src.inference.SUMMARIZER_DICT's keys.
    :param n_sim_docs: int
        number of similar documents to retrieve.
    :param max_summary_tokens: int
        maximum number of tokens in the retrieved documents' summaries.
    :param metrics: List[str]
        metrics to use for evaluation. Should be one of src.evaluation.AVAILABLE_METRICS.
    :param mode: str
        evaluation mode to use. Should be one of AVAILABLE_MODES.
    :param size: int, defaults to 10
        size of the dataset to generate when running in self-supervised mode.
    :param n_sample_sentences: int, defaults to 2
        number of sentences to sample for creating input paragraphs when running in self-supervised mode.
    :param dataset_path: str, defaults to None
        path to the evaluation dataset when running in supervised mode.
    :param experiment_id: str, defaults to "0"
        id of the mlflow experiment to log the results under.
    """
    assert mode in AVAILABLE_MODES, f"mode should be one of {AVAILABLE_MODES}"

    # download the documents from the bucket
    logging.info("Downloading documents")
    download_documents(
        bucket_name=config["bucket_name"],
        destination=config["local_docs_path"],
    )

    if mode == "self-supervised":
        # in self-supervised mode, the ground truth is randomly generated
        logging.info("Generating a random evaluation dataset")
        dataset = generate_dataset(
            document_folder=config["local_docs_path"],
            size=size,
            n_sample_docs=n_sim_docs,
            n_sample_sentences=n_sample_sentences,
        )
    if mode == "supervised":
        # in supervised mode, the ground truth is loaded from a json file
        assert dataset_path is not None, "In supervised mode, dataset_path parameter should be provided."
        logging.info("Loading the evaluation dataset")
        dataset = json.load(open(dataset_path, "r"))

    # create the inference pipeline and a dataframe to store the predictions' scores
    logging.info("Initializing the inference pipeline")
    pipeline = InferencePipeline(
        document_folder=config["local_docs_path"],
        retriever_name=retriever_name,
        summarizer_name=summarizer_name,
        summarizer_kwargs={"max_summary_tokens": max_summary_tokens},
    )
    df_score = pd.DataFrame()

    with mlflow.start_run(experiment_id=experiment_id):
        logging.info("Starting mlflow run")
        # log run parameters with mlflow
        mlflow.log_params(
            {
                "retriever_name": retriever_name,
                "summarizer_name": summarizer_name,
                "n_sim_docs": n_sim_docs,
                "max_summary_tokens": max_summary_tokens,
                "mode": mode,
                "size": size,
                "n_sample_sentences": n_sample_sentences,
                "dataset_path": dataset_path,
            }
        )
        t0 = time.time()
        # for each item of the dataset, compute the pipeline's prediction and evaluate it
        for i, item in tqdm(enumerate(dataset), total=len(dataset)):
            logging.info(f"Starting prediction {i+1}/{len(dataset)}")
            output = pipeline.predict(input_text=item["input_text"], n_sim_docs=n_sim_docs)
            logging.info("Evaluating prediction")
            score = evaluate(y_pred=output, y_true=item["expected_output"], metrics=metrics)
            df_score = pd.concat([df_score, pd.DataFrame([score])]).reset_index(drop=True)
        t1 = time.time()
        # log the mean scores and execution time with mlflow
        mlflow.log_metrics({"execution_time": t1 - t0})
        mlflow.log_metrics({metric: df_score[metric].mean() for metric in metrics})
        # log the dataset used and the detailed scores as well
        with tempfile.TemporaryDirectory() as temp_dir:
            if mode == "self-supervised":
                dataset_path = f"{temp_dir}/dataset.json"
                json.dump(dataset, open(dataset_path, "w"))
            df_path = f"{temp_dir}/scores.csv"
            df_score.to_csv(df_path, index=False, sep=";")
            mlflow.log_artifact(dataset_path, "dataset")
            mlflow.log_artifact(df_path, "scores")


if __name__ == "__main__":
    parser = ArgumentParser()
    parser.add_argument("--retriever-name", type=str, default=config["retriever_name"])
    parser.add_argument("--summarizer-name", type=str, default=config["summarizer_name"])
    parser.add_argument("--n-sim-docs", type=int, default=config["n_sim_docs"])
    parser.add_argument("--max-summary-tokens", type=int, default=config["max_summary_tokens"])
    parser.add_argument("--metrics", type=str, default="similarity,accuracy,rouge_score")
    parser.add_argument("--mode", type=str, default="self-supervised")
    parser.add_argument("--size", type=int, default=10)
    parser.add_argument("--n-sample-sentences", type=int, default=2)
    parser.add_argument("--dataset-path", type=str, default=None)
    parser.add_argument("--experiment-name", type=str, default="dev")

    args = parser.parse_args()
    args.metrics = args.metrics.split(",")
    args_dict = vars(args)

    # If there is already an mlflow experiment under the name specified retrieve its id, else create a new experiment
    experiment_name = args_dict.pop("experiment_name")
    try:
        exp_id = mlflow.get_experiment_by_name(experiment_name).experiment_id
    except AttributeError:
        exp_id = mlflow.create_experiment(experiment_name)

    # Run the pipeline and store the results with mlflow
    evaluation_pipeline(experiment_id=exp_id, **args_dict)
